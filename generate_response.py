# -*- coding: utf-8 -*-
"""generate_response.py - GÃ©nÃ©ration de rÃ©ponses client avec fallbacks robustes"""

import os

# DÃ©tecte si on veut Ã©viter de charger le vrai modÃ¨le (mode tests CI)
SKIP_MODEL = os.environ.get("SKIP_MODEL_DOWNLOAD", "false").lower() in ("1", "true", "yes")

# Variables globales pour le modÃ¨le
tokenizer = None
model = None
gen_pipe = None

def load_model():
    """Charge le modÃ¨le avec des fallbacks robustes"""
    global tokenizer, model, gen_pipe, SKIP_MODEL
    
    if SKIP_MODEL:
        return False
        
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
        
        # ModÃ¨les par ordre de prÃ©fÃ©rence (du plus lÃ©ger au plus complexe)
        model_options = [
            "distilgpt2",           # TrÃ¨s lÃ©ger, trÃ¨s compatible
            "gpt2",                 # LÃ©ger, trÃ¨s compatible  
            "microsoft/DialoGPT-small"  # SpÃ©cialisÃ© dialogue
        ]
        
        for model_name in model_options:
            try:
                print(f"ğŸ¤– Tentative de chargement: {model_name}")
                
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                model = AutoModelForCausalLM.from_pretrained(model_name)
                
                # Configuration du tokenizer
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                
                gen_pipe = pipeline(
                    "text-generation",
                    model=model,
                    tokenizer=tokenizer,
                    max_length=150,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
                
                print(f"âœ… ModÃ¨le {model_name} chargÃ© avec succÃ¨s")
                return True
                
            except Exception as e:
                print(f"âŒ Ã‰chec {model_name}: {str(e)[:100]}...")
                continue
        
        print("âŒ Aucun modÃ¨le disponible, utilisation du mode fallback")
        SKIP_MODEL = True
        return False
        
    except Exception as e:
        print(f"âŒ Erreur import transformers: {e}")
        SKIP_MODEL = True
        return False

# Tentative de chargement au dÃ©marrage
if not SKIP_MODEL:
    load_model()

def generer_reponse(texte, sentiment="negative"):
    """
    GÃ©nÃ¨re une rÃ©ponse polie au client.
    Utilise des templates prÃ©dÃ©finis si le modÃ¨le n'est pas disponible.
    """
    
    # MODE FALLBACK: RÃ©ponses prÃ©dÃ©finies intelligentes
    if SKIP_MODEL or gen_pipe is None:
        
        # Analyser le sentiment pour personnaliser
        if any(word in sentiment.lower() for word in ["positif", "positive", "good", "great"]):
            return (
                f"Merci beaucoup pour votre retour positif ! "
                f"Nous sommes ravis que notre produit vous satisfasse. "
                f"Votre satisfaction est notre prioritÃ©. "
                f"N'hÃ©sitez pas Ã  nous recontacter si vous avez des questions."
            )
        
        elif any(word in sentiment.lower() for word in ["neutre", "neutral", "mixed"]):
            return (
                f"Merci pour votre retour constructif. "
                f"Nous prenons tous les commentaires en considÃ©ration "
                f"pour amÃ©liorer continuellement nos services. "
                f"Votre avis nous aide Ã  mieux vous servir."
            )
        
        else:  # Sentiment nÃ©gatif
            return (
                f"Nous vous remercions pour votre retour et nous excusons "
                f"sincÃ¨rement pour les dÃ©sagrÃ©ments rencontrÃ©s. "
                f"Votre expÃ©rience nous tient Ã  cÅ“ur et nous allons examiner "
                f"votre situation avec attention pour trouver une solution rapide."
            )
    
    # MODE IA: GÃ©nÃ©ration avec modÃ¨le
    try:
        # Prompt simple et efficace
        context = f"Message client: {texte[:200]}"
        
        result = gen_pipe(
            context,
            max_length=len(context) + 100,
            num_return_sequences=1,
            temperature=0.6
        )
        
        generated = result[0]["generated_text"]
        
        # Nettoyer la rÃ©ponse gÃ©nÃ©rÃ©e
        if context in generated:
            response = generated.replace(context, "").strip()
            if len(response) > 20:  # RÃ©ponse valide
                return response[:300]  # Limiter la taille
    
    except Exception as e:
        print(f"Erreur gÃ©nÃ©ration IA: {e}")
    
    # FALLBACK FINAL en cas d'erreur IA
    return (
        "Merci pour votre message. Nous avons bien reÃ§u vos commentaires "
        "et notre Ã©quipe vous recontactera rapidement pour vous assister."
    )


# Test de la fonction
if __name__ == "__main__":
    print("ğŸ§ª Test des rÃ©ponses...")
    
    tests = [
        ("Produit fantastique, trÃ¨s satisfait!", "positif"),
        ("ProblÃ¨me de livraison, trÃ¨s dÃ©Ã§u", "negatif"),
        ("Produit correct mais peut mieux faire", "neutre")
    ]
    
    for texte, sentiment in tests:
        response = generer_reponse(texte, sentiment)
        print(f"\nğŸ“ Texte: {texte}")
        print(f"ğŸ˜Š Sentiment: {sentiment}")
        print(f"ğŸ’¬ RÃ©ponse: {response}")
