# -*- coding: utf-8 -*-
"""
Module de gÃ©nÃ©ration de rÃ©ponses client
Utilise la configuration centralisÃ©e et gÃ¨re les fallbacks robustes
"""

import os
import random
from config.settings import AIConfig, ResponseTemplates

# Variables globales pour le modÃ¨le IA
tokenizer = None
model = None
gen_pipe = None

# Chemin vers le modÃ¨le entraÃ®nÃ©
TRAINED_MODEL_PATH = "./models/amazon_trained"

def load_trained_model():
    """Charge le modÃ¨le entraÃ®nÃ© Amazon si disponible"""
    global tokenizer, model, gen_pipe
    
    from pathlib import Path
    
    if Path(TRAINED_MODEL_PATH).exists():
        try:
            from transformers import GPT2Tokenizer, GPT2LMHeadModel
            
            print(f"ğŸ¯ Chargement du modÃ¨le entraÃ®nÃ© Amazon: {TRAINED_MODEL_PATH}")
            
            tokenizer = GPT2Tokenizer.from_pretrained(TRAINED_MODEL_PATH)
            model = GPT2LMHeadModel.from_pretrained(TRAINED_MODEL_PATH)
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            print("âœ… ModÃ¨le Amazon entraÃ®nÃ© chargÃ© avec succÃ¨s!")
            return True
            
        except Exception as e:
            print(f"âŒ Erreur chargement modÃ¨le entraÃ®nÃ©: {e}")
            return False
    
    return False

def load_ai_model():
    """
    Charge le modÃ¨le IA avec prioritÃ© au modÃ¨le entraÃ®nÃ©
    """
    global tokenizer, model, gen_pipe
    
    if not AIConfig.ENABLE_AI_MODEL:
        print("ğŸ’¬ Mode fallback activÃ© par configuration")
        return False
    
    # PrioritÃ© 1: ModÃ¨le entraÃ®nÃ© Amazon
    if load_trained_model():
        return True
        
    # PrioritÃ© 2: ModÃ¨les prÃ©-entraÃ®nÃ©s
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
        
        print("ğŸ¤– Tentative de chargement des modÃ¨les prÃ©-entraÃ®nÃ©s...")
        
        for model_name in AIConfig.FALLBACK_MODELS:
            try:
                print(f"   Essai: {model_name}")
                
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                model = AutoModelForCausalLM.from_pretrained(model_name)
                
                # Configuration du tokenizer
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                
                gen_pipe = pipeline(
                    "text-generation",
                    model=model,
                    tokenizer=tokenizer,
                    max_length=AIConfig.MAX_LENGTH,
                    temperature=AIConfig.TEMPERATURE,
                    do_sample=AIConfig.DO_SAMPLE,
                    pad_token_id=tokenizer.eos_token_id
                )
                
                print(f"âœ… ModÃ¨le {model_name} chargÃ© avec succÃ¨s")
                return True
                
            except Exception as e:
                print(f"âŒ Ã‰chec {model_name}: {str(e)[:100]}...")
                continue
        
        print("âŒ Aucun modÃ¨le IA disponible, utilisation du mode fallback")
        return False
        
    except ImportError:
        print("ğŸ“¦ Transformers non disponible, utilisation du mode fallback")
        return False
    except Exception as e:
        print(f"âŒ Erreur gÃ©nÃ©rale IA: {e}")
        return False

def generate_fallback_response(texte: str, sentiment: str) -> str:
    """
    GÃ©nÃ¨re une rÃ©ponse prÃ©dÃ©finie basÃ©e sur le sentiment
    
    Args:
        texte: Le texte du client (non utilisÃ© dans le fallback, mais gardÃ© pour compatibilitÃ©)
        sentiment: Le sentiment dÃ©tectÃ© ("positive", "negative", "neutral")
    
    Returns:
        Une rÃ©ponse appropriÃ©e au sentiment
    """
    
    # SÃ©lectionner le bon template selon le sentiment
    if any(word in sentiment.lower() for word in ["positif", "positive", "good", "great"]):
        templates = ResponseTemplates.POSITIVE
    elif any(word in sentiment.lower() for word in ["neutre", "neutral", "mixed"]):
        templates = ResponseTemplates.NEUTRAL
    else:  # Sentiment nÃ©gatif ou inconnu
        templates = ResponseTemplates.NEGATIVE
    
    # Choisir un template alÃ©atoire pour varier les rÃ©ponses
    return random.choice(templates)

def generate_ai_response(texte: str, sentiment: str) -> str:
    """
    GÃ©nÃ¨re une rÃ©ponse avec le modÃ¨le IA (prioritÃ© au modÃ¨le entraÃ®nÃ©)
    """
    global tokenizer, model, gen_pipe
    
    if tokenizer is None or model is None:
        return generate_fallback_response(texte, sentiment)
    
    try:
        # Format d'entrÃ©e
        input_text = f"Avis client: {texte}"
        
        # Tokenisation
        inputs = tokenizer.encode(input_text, return_tensors="pt")
        
        # GÃ©nÃ©ration avec le modÃ¨le
        import torch
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=inputs.shape[1] + 60,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                num_return_sequences=1,
                repetition_penalty=1.2
            )
        
        # DÃ©coder la rÃ©ponse
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extraire la partie rÃ©ponse
        response = generated_text[len(input_text):].strip()
        
        # Nettoyer et valider
        if len(response) > 15 and len(response) < 300:
            from pathlib import Path
            if Path(TRAINED_MODEL_PATH).exists():
                print("ğŸ¯ RÃ©ponse du modÃ¨le entraÃ®nÃ© Amazon")
            else:
                print("ğŸ¤– RÃ©ponse du modÃ¨le prÃ©-entraÃ®nÃ©")
            return response
        else:
            return generate_fallback_response(texte, sentiment)
    
    except Exception as e:
        print(f"âš ï¸ Erreur gÃ©nÃ©ration IA: {e}")
        return generate_fallback_response(texte, sentiment)

def generer_reponse(texte: str, sentiment: str = "negative") -> str:
    """
    Point d'entrÃ©e principal pour gÃ©nÃ©rer une rÃ©ponse client
    
    Args:
        texte: Le texte/avis du client
        sentiment: Le sentiment dÃ©tectÃ© ("positive", "negative", "neutral")
    
    Returns:
        Une rÃ©ponse appropriÃ©e et professionnelle
    """
    
    # Mode IA activÃ© et modÃ¨le disponible
    if AIConfig.ENABLE_AI_MODEL and gen_pipe is not None:
        return generate_ai_response(texte, sentiment)
    
    # Mode fallback (par dÃ©faut)
    return generate_fallback_response(texte, sentiment)

# Initialisation au chargement du module
print("ğŸš€ Initialisation du module de gÃ©nÃ©ration de rÃ©ponses...")
ai_loaded = load_ai_model()
if ai_loaded:
    print("âœ… ModÃ¨le IA chargÃ© et prÃªt")
else:
    print("ğŸ’¬ Mode fallback activÃ© - RÃ©ponses prÃ©dÃ©finies")

# Test de la fonction si exÃ©cutÃ©e directement
if __name__ == "__main__":
    print("\nğŸ§ª Test des rÃ©ponses...")
    
    tests = [
        ("Produit fantastique, trÃ¨s satisfait!", "positif"),
        ("ProblÃ¨me de livraison, trÃ¨s dÃ©Ã§u", "negatif"),
        ("Produit correct mais peut mieux faire", "neutre")
    ]
    
    for texte, sentiment in tests:
        response = generer_reponse(texte, sentiment)
        print(f"\nğŸ“ Texte: {texte}")
        print(f"ğŸ˜Š Sentiment: {sentiment}")
        print(f"ğŸ’¬ RÃ©ponse: {response}")