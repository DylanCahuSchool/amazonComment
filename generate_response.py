# -*- coding: utf-8 -*-
"""
Module de gÃ©nÃ©ration de rÃ©ponses client
Utilise la configuration centralisÃ©e et gÃ¨re les fallbacks robustes
"""

import os
import random
from config.settings import AIConfig, ResponseTemplates

# Variables globales pour le modÃ¨le IA
tokenizer = None
model = None
gen_pipe = None

def load_ai_model():
    """
    Charge le modÃ¨le IA avec des fallbacks robustes
    Retourne True si un modÃ¨le a Ã©tÃ© chargÃ©, False sinon
    """
    global tokenizer, model, gen_pipe
    
    if not AIConfig.ENABLE_AI_MODEL:
        print("ğŸ’¬ Mode fallback activÃ© par configuration")
        return False
        
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
        
        print("ğŸ¤– Tentative de chargement des modÃ¨les IA...")
        
        for model_name in AIConfig.FALLBACK_MODELS:
            try:
                print(f"   Essai: {model_name}")
                
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                model = AutoModelForCausalLM.from_pretrained(model_name)
                
                # Configuration du tokenizer
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                
                gen_pipe = pipeline(
                    "text-generation",
                    model=model,
                    tokenizer=tokenizer,
                    max_length=AIConfig.MAX_LENGTH,
                    temperature=AIConfig.TEMPERATURE,
                    do_sample=AIConfig.DO_SAMPLE,
                    pad_token_id=tokenizer.eos_token_id
                )
                
                print(f"âœ… ModÃ¨le {model_name} chargÃ© avec succÃ¨s")
                return True
                
            except Exception as e:
                print(f"âŒ Ã‰chec {model_name}: {str(e)[:100]}...")
                continue
        
        print("âŒ Aucun modÃ¨le IA disponible, utilisation du mode fallback")
        return False
        
    except ImportError:
        print("ğŸ“¦ Transformers non disponible, utilisation du mode fallback")
        return False
    except Exception as e:
        print(f"âŒ Erreur gÃ©nÃ©rale IA: {e}")
        return False

def generate_fallback_response(texte: str, sentiment: str) -> str:
    """
    GÃ©nÃ¨re une rÃ©ponse prÃ©dÃ©finie basÃ©e sur le sentiment
    
    Args:
        texte: Le texte du client (non utilisÃ© dans le fallback, mais gardÃ© pour compatibilitÃ©)
        sentiment: Le sentiment dÃ©tectÃ© ("positive", "negative", "neutral")
    
    Returns:
        Une rÃ©ponse appropriÃ©e au sentiment
    """
    
    # SÃ©lectionner le bon template selon le sentiment
    if any(word in sentiment.lower() for word in ["positif", "positive", "good", "great"]):
        templates = ResponseTemplates.POSITIVE
    elif any(word in sentiment.lower() for word in ["neutre", "neutral", "mixed"]):
        templates = ResponseTemplates.NEUTRAL
    else:  # Sentiment nÃ©gatif ou inconnu
        templates = ResponseTemplates.NEGATIVE
    
    # Choisir un template alÃ©atoire pour varier les rÃ©ponses
    return random.choice(templates)

def generate_ai_response(texte: str, sentiment: str) -> str:
    """
    GÃ©nÃ¨re une rÃ©ponse avec le modÃ¨le IA
    
    Args:
        texte: Le texte du client
        sentiment: Le sentiment dÃ©tectÃ©
    
    Returns:
        Une rÃ©ponse gÃ©nÃ©rÃ©e par IA ou fallback en cas d'erreur
    """
    if gen_pipe is None:
        return generate_fallback_response(texte, sentiment)
    
    try:
        # Prompt optimisÃ© pour les modÃ¨les gÃ©nÃ©riques
        context = f"Message client: {texte[:200]} - RÃ©ponse service client:"
        
        result = gen_pipe(
            context,
            max_length=len(context) + 80,
            num_return_sequences=1,
            temperature=0.6
        )
        
        generated = result[0]["generated_text"]
        
        # Extraire la rÃ©ponse gÃ©nÃ©rÃ©e
        if context in generated:
            response = generated.replace(context, "").strip()
            if len(response) > 20:  # RÃ©ponse valide
                return response[:300]  # Limiter la taille
    
    except Exception as e:
        print(f"âš ï¸ Erreur gÃ©nÃ©ration IA: {e}")
    
    # Fallback en cas d'erreur IA
    return generate_fallback_response(texte, sentiment)

def generer_reponse(texte: str, sentiment: str = "negative") -> str:
    """
    Point d'entrÃ©e principal pour gÃ©nÃ©rer une rÃ©ponse client
    
    Args:
        texte: Le texte/avis du client
        sentiment: Le sentiment dÃ©tectÃ© ("positive", "negative", "neutral")
    
    Returns:
        Une rÃ©ponse appropriÃ©e et professionnelle
    """
    
    # Mode IA activÃ© et modÃ¨le disponible
    if AIConfig.ENABLE_AI_MODEL and gen_pipe is not None:
        return generate_ai_response(texte, sentiment)
    
    # Mode fallback (par dÃ©faut)
    return generate_fallback_response(texte, sentiment)

# Initialisation au chargement du module
print("ğŸš€ Initialisation du module de gÃ©nÃ©ration de rÃ©ponses...")
ai_loaded = load_ai_model()
if ai_loaded:
    print("âœ… ModÃ¨le IA chargÃ© et prÃªt")
else:
    print("ğŸ’¬ Mode fallback activÃ© - RÃ©ponses prÃ©dÃ©finies")

# Test de la fonction si exÃ©cutÃ©e directement
if __name__ == "__main__":
    print("\nğŸ§ª Test des rÃ©ponses...")
    
    tests = [
        ("Produit fantastique, trÃ¨s satisfait!", "positif"),
        ("ProblÃ¨me de livraison, trÃ¨s dÃ©Ã§u", "negatif"),
        ("Produit correct mais peut mieux faire", "neutre")
    ]
    
    for texte, sentiment in tests:
        response = generer_reponse(texte, sentiment)
        print(f"\nğŸ“ Texte: {texte}")
        print(f"ğŸ˜Š Sentiment: {sentiment}")
        print(f"ğŸ’¬ RÃ©ponse: {response}")